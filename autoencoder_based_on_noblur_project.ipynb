{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742ac1d3-2322-4475-803d-e02fe40bf6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 00:52:38.964052: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-23 00:52:38.964208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-23 00:52:39.174545: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-23 00:52:39.320765: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-23 00:52:49.936208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "from helper_functions import get_picture_filenames_from_folder, load_and_transform_pictures, display_blur_sharp_and_pred_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00398162-433d-4de9-8a6f-425087553b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc65fb2-e3e3-4b40-a086-089a4fe4d3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4acefd-b497-43ea-9a8c-cc08c543794c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path helpers\n",
    "blurred_train_path = os.path.join('clean_data', 'train', 'blur')\n",
    "sharp_train_path = os.path.join('clean_data', 'train', 'sharp')\n",
    "blurred_test_path = os.path.join('clean_data', 'test', 'blur')\n",
    "sharp_test_path = os.path.join('clean_data', 'test', 'sharp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd0905b-2100-4d3e-a53f-4203627b7610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data generator class\n",
    "class DataGenerator(Sequence):\n",
    "    # Based on https://github.com/AryanSethi/No-Blur/blob/master/model_exps/try4.ipynb\n",
    "    \n",
    "    def __init__(self, base_dir,base_dir2, output_size, shuffle=False, batch_size=10):\n",
    "        self.base_dir = base_dir\n",
    "        self.base_dir2 = base_dir2\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.all_x = [f for f in os.listdir(base_dir) if f.endswith(\".png\")]\n",
    "        self.all_y = [f for f in os.listdir(base_dir2) if f.endswith(\".png\")]\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.all_x))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.all_x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, *self.output_size, 3))\n",
    "        Y = np.empty((self.batch_size, *self.output_size, 3))\n",
    "\n",
    "        indices = self.indices[idx*(self.batch_size): (idx+1)*(self.batch_size)]\n",
    "\n",
    "        for i,j in enumerate(indices):\n",
    "            img_path = os.path.join(self.base_dir,self.all_x[j])\n",
    "            img_path2 = os.path.join(self.base_dir2,self.all_y[j])\n",
    "\n",
    "            img  = cv2.imread(img_path)\n",
    "            img2 = cv2.imread(img_path2)\n",
    "            X[i,] = img\n",
    "            Y[i,] = img2\n",
    "        X= X.astype('float32')/255\n",
    "        Y= Y.astype('float32')/255\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c52a9c2-46d2-442d-bd6c-c874ae035faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This autoencoder structure is based on https://github.com/AryanSethi/No-Blur/blob/master/model_exps/try4.ipynb\n",
    "\n",
    "def conv_operation(x, filters, kernel_size, strides=2):\n",
    "    x = layers.Conv2D(filters=filters,\n",
    "              kernel_size=kernel_size,\n",
    "              strides=strides,\n",
    "              padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def conv_transpose_operation(x, filters, kernel_size):\n",
    "    x = layers.Conv2DTranspose(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=2,\n",
    "                       padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def deblurring_autoencoder():\n",
    "    dae_inputs = layers.Input(shape=(720, 1280, 3), name='dae_input')\n",
    "    conv_block1 = conv_operation(dae_inputs, 64, 3)\n",
    "    conv_block2 = conv_operation(conv_block1, 128, 3)\n",
    "    conv_block3 = conv_operation(conv_block2, 256, 3)\n",
    "    conv_block4 = conv_operation(conv_block3, 512, 3)\n",
    "\n",
    "    conv_block5 = conv_operation(conv_block4, 512, 3, 1)\n",
    "\n",
    "    deconv_block1 = conv_transpose_operation(conv_block5, 512,3)\n",
    "    merge1 = layers.Concatenate()([conv_block3,deconv_block1])\n",
    "    deconv_block2 = conv_transpose_operation(merge1, 256, 3)\n",
    "    merge2 = layers.Concatenate()([deconv_block2, conv_block2])\n",
    "    deconv_block3 = conv_transpose_operation(merge2, 128, 3)\n",
    "    merge3 = layers.Concatenate()([deconv_block3, conv_block1])\n",
    "    deconv_block4 = conv_transpose_operation(merge3, 64, 3)\n",
    "\n",
    "    final_deconv = layers.Conv2DTranspose(filters=3, kernel_size=3,padding='same')(deconv_block4)\n",
    "    return Model(dae_inputs, final_deconv, name='dae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8366d1f7-5ade-442a-88d5-8a69a7393145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 00:53:05.062801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30953 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:09:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " dae_input (InputLayer)      [(None, 720, 1280, 3)]       0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 360, 640, 64)         1792      ['dae_input[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 360, 640, 64)         256       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 360, 640, 64)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 180, 320, 128)        73856     ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 180, 320, 128)        512       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 180, 320, 128)        0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 90, 160, 256)         295168    ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 90, 160, 256)         1024      ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 90, 160, 256)         0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 45, 80, 512)          1180160   ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 45, 80, 512)          2048      ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 45, 80, 512)          0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 45, 80, 512)          2359808   ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 45, 80, 512)          2048      ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 45, 80, 512)          0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 90, 160, 512)         2359808   ['re_lu_4[0][0]']             \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 90, 160, 512)         2048      ['conv2d_transpose[0][0]']    \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 90, 160, 512)         0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 90, 160, 768)         0         ['re_lu_2[0][0]',             \n",
      "                                                                     're_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 180, 320, 256)        1769728   ['concatenate[0][0]']         \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 180, 320, 256)        1024      ['conv2d_transpose_1[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 180, 320, 256)        0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 180, 320, 384)        0         ['re_lu_6[0][0]',             \n",
      " )                                                                   're_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 360, 640, 128)        442496    ['concatenate_1[0][0]']       \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 360, 640, 128)        512       ['conv2d_transpose_2[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 360, 640, 128)        0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 360, 640, 192)        0         ['re_lu_7[0][0]',             \n",
      " )                                                                   're_lu[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 720, 1280, 64)        110656    ['concatenate_2[0][0]']       \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 720, 1280, 64)        256       ['conv2d_transpose_3[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 720, 1280, 64)        0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2D  (None, 720, 1280, 3)         1731      ['re_lu_8[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8604931 (32.83 MB)\n",
      "Trainable params: 8600067 (32.81 MB)\n",
      "Non-trainable params: 4864 (19.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = deblurring_autoencoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c66dc27a-d71e-46a3-8cb8-fed57a73dc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model inputs, compile model\n",
    "\n",
    "train = DataGenerator(blurred_train_path, sharp_train_path, (720,1280), batch_size=8, shuffle=False)\n",
    "test  = DataGenerator(blurred_test_path, sharp_test_path, (720,1280), batch_size=8, shuffle=False)\n",
    "opt=Adam(learning_rate=0.001)\n",
    "\n",
    "now = datetime.now()\n",
    "checkpoint_filepath = f\"models/autoencoder_{now.year}_{now.month}_{now.day}T{now.hour}_{now.minute}_epoch\"+\"{epoch:02d}_mae_{loss:.4f}_val_mae_{val_loss:.4f}.keras\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose = 1,\n",
    "    save_best_only=False)\n",
    "\n",
    "model.compile(optimizer=opt, loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31357210-d628-4f71-8abe-b86b06c60a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model to continue training\n",
    "model_name = 'autoencoder_2023_12_22T18_48_epoch55_mae_0.0349_val_mae_0.0333.keras'\n",
    "model_path = 'models'\n",
    "model = load_model(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7529fb-61fd-44d1-a54a-57bea72c6ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 00:53:34.328353: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-23 00:54:02.430352: I external/local_xla/xla/service/service.cc:168] XLA service 0x2ace2071eac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-23 00:54:02.430431: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2023-12-23 00:54:02.446915: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703285642.760206   22139 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/262 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 1: saving model to models/autoencoder_2023_12_23T0_53_epoch01_mae_0.0349_val_mae_0.0369.keras\n",
      "262/262 [==============================] - 544s 2s/step - loss: 0.0349 - val_loss: 0.0369\n",
      "Epoch 2/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 2: saving model to models/autoencoder_2023_12_23T0_53_epoch02_mae_0.0350_val_mae_0.0363.keras\n",
      "262/262 [==============================] - 544s 2s/step - loss: 0.0350 - val_loss: 0.0363\n",
      "Epoch 3/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 3: saving model to models/autoencoder_2023_12_23T0_53_epoch03_mae_0.0344_val_mae_0.0332.keras\n",
      "262/262 [==============================] - 534s 2s/step - loss: 0.0344 - val_loss: 0.0332\n",
      "Epoch 4/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 4: saving model to models/autoencoder_2023_12_23T0_53_epoch04_mae_0.0345_val_mae_0.0332.keras\n",
      "262/262 [==============================] - 568s 2s/step - loss: 0.0345 - val_loss: 0.0332\n",
      "Epoch 5/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 5: saving model to models/autoencoder_2023_12_23T0_53_epoch05_mae_0.0348_val_mae_0.0354.keras\n",
      "262/262 [==============================] - 554s 2s/step - loss: 0.0348 - val_loss: 0.0354\n",
      "Epoch 6/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0341\n",
      "Epoch 6: saving model to models/autoencoder_2023_12_23T0_53_epoch06_mae_0.0341_val_mae_0.0343.keras\n",
      "262/262 [==============================] - 535s 2s/step - loss: 0.0341 - val_loss: 0.0343\n",
      "Epoch 7/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 7: saving model to models/autoencoder_2023_12_23T0_53_epoch07_mae_0.0342_val_mae_0.0348.keras\n",
      "262/262 [==============================] - 519s 2s/step - loss: 0.0342 - val_loss: 0.0348\n",
      "Epoch 8/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 8: saving model to models/autoencoder_2023_12_23T0_53_epoch08_mae_0.0340_val_mae_0.0336.keras\n",
      "262/262 [==============================] - 517s 2s/step - loss: 0.0340 - val_loss: 0.0336\n",
      "Epoch 9/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 9: saving model to models/autoencoder_2023_12_23T0_53_epoch09_mae_0.0340_val_mae_0.0388.keras\n",
      "262/262 [==============================] - 545s 2s/step - loss: 0.0340 - val_loss: 0.0388\n",
      "Epoch 10/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 10: saving model to models/autoencoder_2023_12_23T0_53_epoch10_mae_0.0344_val_mae_0.0369.keras\n",
      "262/262 [==============================] - 539s 2s/step - loss: 0.0344 - val_loss: 0.0369\n",
      "Epoch 11/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 11: saving model to models/autoencoder_2023_12_23T0_53_epoch11_mae_0.0340_val_mae_0.0399.keras\n",
      "262/262 [==============================] - 527s 2s/step - loss: 0.0340 - val_loss: 0.0399\n",
      "Epoch 12/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 12: saving model to models/autoencoder_2023_12_23T0_53_epoch12_mae_0.0339_val_mae_0.0367.keras\n",
      "262/262 [==============================] - 541s 2s/step - loss: 0.0339 - val_loss: 0.0367\n",
      "Epoch 13/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 13: saving model to models/autoencoder_2023_12_23T0_53_epoch13_mae_0.0337_val_mae_0.0387.keras\n",
      "262/262 [==============================] - 546s 2s/step - loss: 0.0337 - val_loss: 0.0387\n",
      "Epoch 14/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 14: saving model to models/autoencoder_2023_12_23T0_53_epoch14_mae_0.0337_val_mae_0.0379.keras\n",
      "262/262 [==============================] - 541s 2s/step - loss: 0.0337 - val_loss: 0.0379\n",
      "Epoch 15/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 15: saving model to models/autoencoder_2023_12_23T0_53_epoch15_mae_0.0339_val_mae_0.0371.keras\n",
      "262/262 [==============================] - 534s 2s/step - loss: 0.0339 - val_loss: 0.0371\n",
      "Epoch 16/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 16: saving model to models/autoencoder_2023_12_23T0_53_epoch16_mae_0.0336_val_mae_0.0393.keras\n",
      "262/262 [==============================] - 518s 2s/step - loss: 0.0336 - val_loss: 0.0393\n",
      "Epoch 17/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 17: saving model to models/autoencoder_2023_12_23T0_53_epoch17_mae_0.0333_val_mae_0.0339.keras\n",
      "262/262 [==============================] - 516s 2s/step - loss: 0.0333 - val_loss: 0.0339\n",
      "Epoch 18/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 18: saving model to models/autoencoder_2023_12_23T0_53_epoch18_mae_0.0333_val_mae_0.0356.keras\n",
      "262/262 [==============================] - 546s 2s/step - loss: 0.0333 - val_loss: 0.0356\n",
      "Epoch 19/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 19: saving model to models/autoencoder_2023_12_23T0_53_epoch19_mae_0.0332_val_mae_0.0340.keras\n",
      "262/262 [==============================] - 531s 2s/step - loss: 0.0332 - val_loss: 0.0340\n",
      "Epoch 20/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 20: saving model to models/autoencoder_2023_12_23T0_53_epoch20_mae_0.0330_val_mae_0.0349.keras\n",
      "262/262 [==============================] - 524s 2s/step - loss: 0.0330 - val_loss: 0.0349\n",
      "Epoch 21/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 21: saving model to models/autoencoder_2023_12_23T0_53_epoch21_mae_0.0330_val_mae_0.0357.keras\n",
      "262/262 [==============================] - 528s 2s/step - loss: 0.0330 - val_loss: 0.0357\n",
      "Epoch 22/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 22: saving model to models/autoencoder_2023_12_23T0_53_epoch22_mae_0.0331_val_mae_0.0370.keras\n",
      "262/262 [==============================] - 522s 2s/step - loss: 0.0331 - val_loss: 0.0370\n",
      "Epoch 23/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 23: saving model to models/autoencoder_2023_12_23T0_53_epoch23_mae_0.0328_val_mae_0.0355.keras\n",
      "262/262 [==============================] - 497s 2s/step - loss: 0.0328 - val_loss: 0.0355\n",
      "Epoch 24/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 24: saving model to models/autoencoder_2023_12_23T0_53_epoch24_mae_0.0328_val_mae_0.0328.keras\n",
      "262/262 [==============================] - 510s 2s/step - loss: 0.0328 - val_loss: 0.0328\n",
      "Epoch 25/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 25: saving model to models/autoencoder_2023_12_23T0_53_epoch25_mae_0.0330_val_mae_0.0337.keras\n",
      "262/262 [==============================] - 509s 2s/step - loss: 0.0330 - val_loss: 0.0337\n",
      "Epoch 26/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 26: saving model to models/autoencoder_2023_12_23T0_53_epoch26_mae_0.0329_val_mae_0.0333.keras\n",
      "262/262 [==============================] - 518s 2s/step - loss: 0.0329 - val_loss: 0.0333\n",
      "Epoch 27/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 27: saving model to models/autoencoder_2023_12_23T0_53_epoch27_mae_0.0327_val_mae_0.0410.keras\n",
      "262/262 [==============================] - 510s 2s/step - loss: 0.0327 - val_loss: 0.0410\n",
      "Epoch 28/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 28: saving model to models/autoencoder_2023_12_23T0_53_epoch28_mae_0.0326_val_mae_0.0340.keras\n",
      "262/262 [==============================] - 513s 2s/step - loss: 0.0326 - val_loss: 0.0340\n",
      "Epoch 29/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 29: saving model to models/autoencoder_2023_12_23T0_53_epoch29_mae_0.0326_val_mae_0.0333.keras\n",
      "262/262 [==============================] - 515s 2s/step - loss: 0.0326 - val_loss: 0.0333\n",
      "Epoch 30/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 30: saving model to models/autoencoder_2023_12_23T0_53_epoch30_mae_0.0325_val_mae_0.0338.keras\n",
      "262/262 [==============================] - 524s 2s/step - loss: 0.0325 - val_loss: 0.0338\n",
      "Epoch 31/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 31: saving model to models/autoencoder_2023_12_23T0_53_epoch31_mae_0.0323_val_mae_0.0341.keras\n",
      "262/262 [==============================] - 517s 2s/step - loss: 0.0323 - val_loss: 0.0341\n",
      "Epoch 32/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 32: saving model to models/autoencoder_2023_12_23T0_53_epoch32_mae_0.0325_val_mae_0.0320.keras\n",
      "262/262 [==============================] - 523s 2s/step - loss: 0.0325 - val_loss: 0.0320\n",
      "Epoch 33/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 33: saving model to models/autoencoder_2023_12_23T0_53_epoch33_mae_0.0323_val_mae_0.0344.keras\n",
      "262/262 [==============================] - 514s 2s/step - loss: 0.0323 - val_loss: 0.0344\n",
      "Epoch 34/100\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 34: saving model to models/autoencoder_2023_12_23T0_53_epoch34_mae_0.0328_val_mae_0.0331.keras\n",
      "262/262 [==============================] - 485s 2s/step - loss: 0.0328 - val_loss: 0.0331\n",
      "Epoch 35/100\n",
      "204/262 [======================>.......] - ETA: 1:23 - loss: 0.0324"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    callbacks = [model_checkpoint_callback],\n",
    "    validation_data = test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfa837-a85e-44d3-9e16-7661a32e0b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4a764-9761-46ce-8a04-e90ef9d646c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_name = 'autoencoder_2023_12_22T18_48_epoch55_mae_0.0349_val_mae_0.0333.keras'\n",
    "model_path = 'models'\n",
    "best_model = load_model(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bd890-781a-4df3-8513-6fd1b25fde32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in data splits for testing\n",
    "num_of_test_files = 10\n",
    "test_batch_filenames = get_picture_filenames_from_folder(blurred_test_path)[:num_of_test_files]\n",
    "blurred_test_batch = load_and_transform_pictures(test_batch_filenames, blurred_test_path)\n",
    "sharp_test_batch = load_and_transform_pictures(test_batch_filenames, sharp_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182b056-c716-4e6a-8278-74ae5bf5ce05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predicting with best model\n",
    "predictions = np.clip(best_model.predict(blurred_test_batch), 0, 1).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e96ad-d1f8-45cd-887d-38d30d5c6c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display blurred, predicted and sharp images\n",
    "img_id = 8\n",
    "display_blur_sharp_and_pred_images(img_id, blurred_test_batch, sharp_test_batch, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ecd449-2f30-41a7-a511-fbe982ac04c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pov_new",
   "language": "python",
   "name": "pov_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
